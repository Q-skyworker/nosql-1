部署Radis集群（默认至少6个节点）
介绍：
Redis 集群是一个分布式（distributed）、容错（fault-tolerant）的 Redis 实现， 集群可以使用的功能是普通单机 Redis 所能使用的功能的一个子集（subset）。
Redis 集群中不存在中心（central）节点或者代理（proxy）节点， 集群的其中一个主要设计目标是达到线性可扩展性（linear scalability）。
Redis 集群为了保证一致性（consistency）而牺牲了一部分容错性： 系统会在保证对网络断线（net split）和节点失效（node failure）具有有限（limited）抵抗力的前提下， 尽可能地保持

数据的一致性。
解决redis单点故障

1.准备六台redis机器（51-56）搭建好radis服务----原厂原装

清空机器：
----------------------------------------------------------------------
进入redis
>keys * 查看
>flushall 
>shutdown(删掉内存,并关闭服务)
>quit
ls /var/lib/redis/6379----redis存放数据的文件（硬盘中）
rm -rf /var/lib/redis/6379/*----删掉硬盘中的文件

等一段时间后再进入
查看是否清空

#########################################################
2.
2.1配置IP
修改IP地址与端口号，不用默认地址，以守护进程进行

关闭redis   -----------------------/etc/init.d/redis_6379 stop
vim打开配置：------------------------------------------vim /etc/redis/6379.conf
815：/cluster---启用集群配置           cluster-enabled yes
823：nodes-6355（端口）.conf        cluster-config-file nodes-6351.conf
829：超时时间5000                         cluster-node-timeout 5000(多久通信集群一次)
更改端口配置，改默认的IP地址              port 6351  /  bind 192.168.4.51

启动服务，查看进程信息（会自动生成集群中的端口）

*******************************************************
2.2创建集群--------------通过ruby脚本（管理：创建，查看，添加，删除）
   创建集群时，保证数据库中没有数据
将 cd redis-4.0.8/src/redis-trib.rb
cp redis-trib.rb  /usr/local/sbin/
做成命令--------ruby脚本

yum -y install ruby rubygems
rpm -ivh --nodeps ruby-devel-2.0.0.648-30.el7.x86_64.rpm 
gem install redis-3.2.1.gem

2.2.2把ruby脚本拷贝到系统命令目录下

2.2.3创建集群  redis-trib.rb help查看创建格式
--replicas 1
192.168.4.51:6351 \
192.168.4.52:6352 \
192.168.4.53:6353 \
192.168.4.54:6354 \
192.168.4.55:6355 \
192.168.4.56:6356


可以看配置文件，看到库的信息
[root@51 ~]# cat /var/lib/redis/6379/nodes-6351.conf 
                   ID                        IP       端口号         状态                  slave的masterID            ---------------查看配置好的主从信息
d18277c456f6a66a77567dbb9a945cd8079c14e4 192.168.4.53:6353@16353 master - 0 1530610230000 3 connected 10923-16383
f762dac1e92c192d537888c29f6adde1f8238732 192.168.4.51:6351@16351 myself,master - 0 1530610230000 1 connected 0-5460
b465b5fa866f6d6a0a5f9a86a8fd387d1fea731b 192.168.4.56:6356@16356 slave 1ffd5d78b906105bca6048bd44616a99f4f8f47d 0 1530610231033 6 connected
1ffd5d78b906105bca6048bd44616a99f4f8f47d 192.168.4.52:6352@16352 master - 0 1530610231000 2 connected 5461-10922
9c0004e77f76cadb4a77556aed2ea4f272068cac 192.168.4.55:6355@16355 slave f762dac1e92c192d537888c29f6adde1f8238732 0 1530610231134 5 connected
4a7137d9b967fcb55e23764e58267daef8fcf06f 192.168.4.54:6354@16354 slave d18277c456f6a66a77567dbb9a945cd8079c14e4 0 1530610229000 4 connected

cluster nodes命令可以查看当前集群的信息

我们连接到集群中的任意一个结点，启动redis-cli时要加-c选项，存取两个Key-Value感受一下Redis久违的集群功能。

redis-cli -c -h 192.168.4.51 -p 6351
---------------------------------------------------------
使用ruby脚本查看信息：


原理：
0-16384个hash slot（哈希槽）
1.将16383个hash slot分给分好的各个主库（一般为平均分配），每一个主库分的一定范围的hash slot
2.将输入的内容与CRC16算法进行计算：
set name
name/CRC16=7  7%16384=4568----->落到哪个槽范围内就是存在谁上面
3.取值为逆过程  
问题：若有5个或者7个库如何分配？

***注意：hash结果只决定在哪里存放，不表示能够存多少个值-------存放的数据只与硬盘与内存有关
****当hash槽改变了之前在从库中的数据不影响现在或在原来的数据存放
****当没有从库的主库挂掉，集群就会挂掉
****每一个主库的数据都只是片面的一个丢失，整个数据就会丢失
###########################################################

测试集群：
1.再客户端访问任意一台master角色的redis查询数据并存储。
2.存，取访问任意的主库都可以（但看不完整）
#################################################
管理集群：
常用选项
– -h IP 地址
– -p 端口
– -c
+++++++++++++++++++++++++++++++++
• 语法格式
– Redis-trib.rb
选项 参数
• 选项
– add-node 添加新节点（默认主库）
– check 对节点主机做检查           -------- redis-trib.rb check 192.168.4.51:6351 检测集群状态
– reshard 对节点主机重新分片
– add-node --slave 添加从节点主机
– del-node 删除节点主机
+++++++++++++++++++++++++++++++++++++

集群节点选举测试
如何选举 master 主机
• 把是 master 角色主机上的 Redis 服务 停止
#redis-cli -h ip -p 端口 shutdown  
主库down掉后从变为主，修好后就变成从

################################################################

添加新的redis服务器：
1.添加master主机
   准备干净的主机50---->配置好基本的主机配置redis
redis-trib.rb add-node 新主机 Ip: 端口 192.168.4.51:6351
   将51上添加新机50
redis-trib.rb add-node  192.168.4.50:6350(要添加的) 192.168.4.51:6351（存在的）
   添加后50为master但没有hash槽
分片：
redis-trib.rb reshard 192.168.4.51:6351
How many slots do you want to move (from 1 to 16384)? 4096（一般为均分）
Source node #1:all---（从所有的hash槽中抽取）
yes确认
-------------------------------------------------------------------------------------

2.添加slave
redis-trib.rb add-node --slave  192.168.4.57:6357(添加的从库)  192.168.4.51:6351（当前的库）
默认谁的从库最少就添加谁，一样的情况随机分配，也可以自定义某个主库--master ID
redis-trib.rb add-node --slave --master-id 1ffd5d78b906105bca6048bd44616a99f4f8f47d（主库的ID）  192.168.4.58:6358（从库） 192.168.4.51:6351（）
指定添加的从库
*************************************************************

3.删除节点：
1.移除从库 redis-trib.rb del-node 192.168.4.51:6351（任意的库） cb7c16b822566894499618f5bf32164bdf586bf1（要移除的从库ID）

2.移除主库:
    1  分片将50的hash槽移动除去  redis-trib.rb reshard 192.168.4.51:6351
	How many slots do you want to move (from 1 to 16384)? 4096
	What is the receiving node ID? f762dac1e92c192d537888c29f6adde1f8238732（接收hash槽的ID）
	Please enter all the source node IDs.
	输入要被移除的主库ID，确认用done
    2  将50删除
---------------------------------------------------------------------------------------------------------
***************************************************************
如何将移除的主机再次添加到集群中：

**注意：问题50 在清空数据后，又再次添加到集群中的时候：无法添加到集群？
      解释：保留了node号，集群会认为这是一个已经存在的主库，不能添加。因此需要在恢复cluster配置后，在50上重新cluster reset---->使cluster info为fail
才可以将50再次添加到集群中。主库，从库均是这种情况。

+++++++++++++++++++++++++++++++++++++++++++
移除库---变成一个独立的库
1.关服务
2.改回配置文件
3.删除相关的数据库文件/var/lib/resdis/6379/*
4.重启服务


Master可读可写，Slaver只能读，不能写
Master可以对应多个Slaver，但是数量越多压力越大，延迟就可能越严重
Master写入后立即返回，几乎同时将写入异步同步到各个Slaver，所以基本上延迟可以忽略
可以通过slaveof no one命令将Slaver升级为Master（当Master挂掉时，手动将某个Slaver变为Master）
可以通过sentinel哨兵模式监控Master，当Master挂掉时自动选举Slaver变为Master，其它Slaver自动重连新的Master

###############################################################################


redis集群提供了以下两个好处redis-trib.rb del-node 192.168.4.51:6351 cb7c16b822566894499618f5bf32164bdf586bf1
1、将数据自动切分(split)到多个节点
2、当集群中的某一个节点故障时，redis还可以继续处理客户端的请求。
一个 redis 集群包含 16384 个哈希槽（hash slot），数据库中的每个数据都属于这16384个哈希槽中的一个。集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽。集群中的每一个节点负责处理一部分哈希槽。
集群中的主从复制
集群中的每个节点都有1个至N个复制品，其中一个为主节点，其余的为从节点，如果主节点下线了，集群就会把这个主节点的一个从节点设置为新的主节点，继续工作。这样集群就不会因为一个主节点的下线而无法正常工作
注意：
1、如果某一个主节点和他所有的从节点都下线的话，redis集群就会停止工作了。redis集群不保证数据的强一致性，在特定的情况下，redis集群会丢失已经被执行过的写命令
2、使用异步复制（asynchronous replication）是redis 集群可能会丢失写命令的其中一个原因，有时候由于网络原因，如果网络断开时间太长，redis集群就会启用新的主节点，之前发给主节点的数据就会丢失。
****************************************************
   3节点环境：1个master、2个slave
        存储空间：最大等于1个节点的容量。（如果是2个master的话，那么数据会丢失一部分）
        冗余性：允许1个节点故障。
        
    4节点环境：2个master、2个slave
        存储空间：2个节点的容量。
        冗余性：允许1个节点故障。（集群中，半数以上节点认为故障，才会选举。）
    
    5节点环境：2个master、3个slave
        存储空间：2个节点的容量。
        冗余性：允许2个节点故障。
        
    6节点环境：3个master、3个slave
        存储空间：3个节点的容量。
        冗余性：允许2个节点故障。
        
    依次类推... ...
    
    3节点和4节点对比：
        容量：后者多了1个节点的存储空间。
        冗余：都是允许1个节点故障。
        
    4节点和5节点对比：
        容量：都是2个节点的容量。
        冗余：后者多了1个节点的冗余。
            
    通过对比发现： redis集群选择奇数节点还是偶数，还是要看业务的需求。
    
    举个例子：业务的数据总量2个节点就可以满足，那么就看业务对冗余性的要求，
                    如果有1个冗余就OK，那么就考虑4个节点的集群。
                    如果要更高的冗余，那么就考虑5个节点的集群。
                       
    不过话又说回来了，企业既然选择了redis集群，公司也不差这一个节点的钱，
    一般做法是偶数节点的redis集群。

##############################################################

排错点：无法关闭redis----kill pid



